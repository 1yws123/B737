import os
import time
import csv
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from schedulers import WarmupCosineScheduler 

from model import PointCloudVAE
from dataset import SDFDataset

def get_args():
    parser = argparse.ArgumentParser(description='Stage 2: Frozen Encoder Aero Training')
    
    # --- è·¯å¾„é…ç½® ---
    parser.add_argument('--pc_root', type=str, default='/home/yuwenshi/B737/G58_pc_1299/pointcloud')
    parser.add_argument('--aero_root', type=str, default='/home/yuwenshi/B737/G58_aero_1299/G58_aero_1299')
    parser.add_argument('--sdf_dir', type=str, default='/home/yuwenshi/B737/G58_sdf_1299/sdf_data')
    parser.add_argument('--stage1_ckpt', type=str, default='/home/yuwenshi/B737/checkpoint_all_1/vae_epoch_8400.pth')
    parser.add_argument('--save_dir', type=str, default='checkpoints_stage2_3')

    parser.add_argument('--surface_ratio', type=float, default=0.8, help='SDF é‡‡æ ·ä¸­è¡¨é¢ç‚¹çš„æ¯”ä¾‹')
    parser.add_argument('--surface_threshold', type=float, default=0.02, help='è¡¨é¢ç‚¹åˆ¤å®šçš„é˜ˆå€¼')
    parser.add_argument('--num_points_sdf', type=int, default=250000, help='SDF æ¡æ¨£é»ç¸½æ•¸ (Vol + Near)')

    parser.add_argument('--latent_dim', type=int, default=128, help='æ½›åœ¨ç©ºé–“ç¶­åº¦')
    parser.add_argument('--plane_res', type=int, default=128, help='Triplane ç‰¹å¾µå¹³é¢çš„åˆ†è¾¨ç‡ (64 æˆ– 128 å¯æå‡ç´°ç¯€)')
    parser.add_argument('--plane_feat', type=int, default=32, help='Triplane ç‰¹å¾µé€šé“æ•¸')
    parser.add_argument('--fourier_dim', type=int, default=8, help='å‚…ç«‹è‘‰ç‰¹å¾µé »ç‡æ•¸é‡')
    parser.add_argument('--val_ratio', type=float, default=0.2, help="éªŒè¯é›†æ¯”ä¾‹ (0.2 = 20%)")

    # --- è®­ç»ƒå‚æ•° ---
    parser.add_argument('--epochs', type=int, default=3000)
    parser.add_argument('--batch_size', type=int, default=60, help='å› ä¸ºå†»ç»“äº†å¤§éƒ¨åˆ†å‚æ•°,æ˜¾å­˜å ç”¨å°,Batchå¯è°ƒå¤§')
    parser.add_argument('--lr', type=float, default=1e-3)
    
    # --- è®ºæ–‡å…¬å¼å¯¹åº”çš„ç³»æ•° ---
    # CL çº¦ 0.5, CD çº¦ 0.02ã€‚CD çš„ MSE ä¼šéå¸¸å° (1e-4çº§åˆ«)ã€‚
    # alpha=1.0, beta=100.0 (è®© CD çš„æ¢¯åº¦æ”¾å¤§100å€)
    #parser.add_argument('--alpha', type=float, default=1.0, help='Weight for CL')
    #parser.add_argument('--beta', type=float, default=100.0, help='Weight for CD')

    return parser.parse_args()

def validate(model, val_loader, device, criterion):
    model.eval()
    val_loss = 0.0
    val_mae = 0.0
    
    with torch.no_grad():
        for data in val_loader:
            pc = data['point_cloud'].to(device)
            # GT Shape: [B, 2] -> æˆ‘ä»¬åªå–ç¬¬ä¸€åˆ— CL -> [B, 1]
            gt_aero = data['aero_label'].to(device).float()
            gt_cl = gt_aero[:, 0].unsqueeze(1) 

            # Forward
            _, _, _, aero_pred = model(pc) # aero_pred: [B, 1]
            
            # Loss
            loss = criterion(aero_pred, gt_cl)
            val_loss += loss.item()
            
            # MAE
            val_mae += torch.mean(torch.abs(aero_pred - gt_cl)).item()
            
    avg_loss = val_loss / len(val_loader)
    avg_mae = val_mae / len(val_loader)
    return avg_loss, avg_mae

def main():
    args = get_args()
    device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
    if not os.path.exists(args.save_dir): os.makedirs(args.save_dir)
        
    print(f"å¼€å§‹ CL å•ä»»åŠ¡è®­ç»ƒ | Epochs: {args.epochs} | LR: {args.lr}")

    # 1. æ•°æ®é›†
    dataset = SDFDataset(
        surface_ratio=args.surface_ratio,
        surface_threshold=args.surface_threshold,
        pc_root_dir=args.pc_root,
        aero_root_dir=args.aero_root,
        sdf_dir=args.sdf_dir,
        num_points_uniform=4000, num_points_curvature=4000, num_points_importance=4000
    )
    # åˆ’åˆ† è®­ç»ƒé›† / éªŒè¯é›†
    val_size = int(len(dataset) * args.val_ratio)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    print(f"æ•°æ®é›†: æ€»æ•° {len(dataset)} | è®­ç»ƒé›† {train_size} | éªŒè¯é›† {val_size}")
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)

    # 2. æ¨¡å‹åŠ è½½
    model = PointCloudVAE(
       latent_dim=args.latent_dim,
       plane_resolution=args.plane_res,
       plane_features=args.plane_feat,
       num_fourier_freqs=args.fourier_dim,
       num_points_uniform=4000,
       num_points_curvature=4000, # ä¼ å…¥
       num_points_importance=4000
).to(device)
    
    if os.path.exists(args.stage1_ckpt):
        print(f"ğŸ“¥ åŠ è½½ Stage 1 æƒé‡: {args.stage1_ckpt}")
        ckpt = torch.load(args.stage1_ckpt, map_location=device,weights_only=False)
        model.load_state_dict(ckpt, strict=False)
    else:
        raise FileNotFoundError("å¿…é¡»æä¾› Stage 1 æƒé‡æ‰èƒ½è¿›è¡Œå†»ç»“è®­ç»ƒï¼")

    # ========================================================================
    # 3. æ ¸å¿ƒæ­¥éª¤ï¼šå†»ç»“å‡ ä½•å‚æ•° (Freeze Parameters)
    # ========================================================================
    # å†»ç»“ Encoder
    for param in model.encoder.parameters(): param.requires_grad = False

    # å†»ç»“ Triplane Decoder
    for param in model.decoder.parameters(): param.requires_grad = False
    
    # ç¡®ä¿ Aero Decoder å¯è®­ç»ƒ
    for param in model.aero_decoder.parameters(): param.requires_grad = True
        
    print("å·²å†»ç»“ Encoder å’Œ Decoder å‚æ•°ï¼Œä»…è®­ç»ƒ Aero Branch")

    # 4. ä¼˜åŒ–å™¨ï¼šåªä¼ å…¥ aero_decoder çš„å‚æ•°
    optimizer = optim.Adam(model.aero_decoder.parameters(), lr=args.lr)
    scheduler = WarmupCosineScheduler(optimizer, warmup_epochs=args.epochs // 20, total_epochs=args.epochs)
    criterion = nn.MSELoss()
    
    # 5. è®°å½•æ—¥å¿—
    log_file = os.path.join(args.save_dir, 'train_log.csv')
    with open(log_file, 'w', newline='') as f:
        csv.writer(f).writerow(['Epoch', 'Train_Loss', 'Val_Loss', 'Val_MAE', 'LR'])

    # 6. è®­ç»ƒå¾ªç¯
    best_val_mae = float('inf')
    
    for epoch in range(args.epochs):
        model.train() # è®­ç»ƒæ¨¡å¼ (å¯¹äºå†»ç»“å±‚ï¼ŒDropoutè¡Œä¸ºå–å†³äºå®ç°ï¼Œé€šå¸¸å»ºè®®BN evalæ¨¡å¼)
        model.aero_decoder.train() # ç¡®ä¿ decoder æ˜¯ train
        epoch_loss = 0
        
        for i, data in enumerate(train_loader):
            pc = data['point_cloud'].to(device)
            # æ ‡ç­¾å¤„ç†: åªå– CL
            gt_aero = data['aero_label'].to(device).float()
            gt_cl = gt_aero[:, 0].unsqueeze(1) # [B, 1]

            optimizer.zero_grad()
            
            # Forward
            _, _, _, aero_pred = model(pc)
            
            loss = criterion(aero_pred, gt_cl)
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        # æ›´æ–°å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step()
        avg_train_loss = epoch_loss / len(train_loader)

        # === éªŒè¯ ===
        # æ¯ 200 è½®(æˆ–è€…æ˜¯æœ€åå‡ è½®è¿›è¡ŒéªŒè¯)
        if (epoch + 1) % 1 == 0 :
            val_loss, val_mae = validate(model, val_loader, device, criterion)
            
            print(f"Epoch {epoch+1:4d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {val_loss:.6f} | Val MAE: {val_mae:.4f} | LR: {current_lr:.6f}")
            
            # è®°å½•æ—¥å¿—
            with open(log_file, 'a', newline='') as f:
                csv.writer(f).writerow([epoch+1, avg_train_loss, val_loss, val_mae, current_lr])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_mae < best_val_mae:
                best_val_mae = val_mae
                torch.save(model.state_dict(), os.path.join(args.save_dir, 'best_cl_model.pth'))
                print(f"Best Model Saved (MAE: {best_val_mae:.4f})")
        else:
            # ç®€ç•¥æ‰“å°
            print(f"\rEpoch {epoch+1:4d} | Train Loss: {avg_train_loss:.6f}", end="")

    print(f"\nè®­ç»ƒç»“æŸ! æœ€ä½³éªŒè¯é›† MAE: {best_val_mae:.4f}")

if __name__ == "__main__":
    main()